# Large Language Models, pt1

Before we talk about using LLMs, let's talk about
what they are and how to run them.

## What is an LLM?

- A big pile of neural network.

Large language model steps:

### Tokens

- Neural networks like numbers.
- We have words.
- We transform words into numbers with a dictionary.
- Typical dictionary size: 100k
- (There are maybe 30k common words in English)

Ex:
  aardvark -> 0
  abacus -> 1
  abandon -> 2
  ...
  -s (plural)  -> 39952

Aardvarks abandon abacuses -> 0 39952 2 1 39952

### Tensors

- A Scalar is one value (e.g. 7)
- A vector is a 1D sequence (array) of values.
- A matrix is a 2D array of values.
- A tensor is an N-D array of values.

### Neural Nets

- A DAG with nodes.
- Each node has a weight (or tensor of weights).
- A weight is typically a floating point number normalized
  to the range -1 .. 1.
- Nodes also have operation.

### Embeddings

- One chunk of neural network translates token numbers
  into Embeddings.
- An embedding is a tensor representation of a token with
  some represnetaoitn of "meaning".
- Example: "wolf" and "dog" will have similar embeddings,
  but "wolf" will be more like "wild animal" and "dog" will
  be more like "pet".
- Position in input text is also represented in the
  embedding tensor.

### Next Token Prediction

- After the token -> embedding network, the next chunk of
  neural network is to do next token prediction.
- The output is one token.

### Training

- Take a training text.
- Feed the first token into the LLM, trying to predict
  the second token.
- Calculate difference between actual output and correct
  output.
- Calculate the partial differental between the output
  of each node and the output that would have gotten us
  the correct prediction.
- Change the weights very slightly in the right direction.

### Inference

Once the network is trained, we can predict next token just
by running the network on the input sequence.

## How big?

- How many weights?
- Each weight is a floating point number, normalized to -1..1
- Standard "full precision" for weights in 2025 is 16 bit floats,
  or two bytes per weight.
- So if you see a "10B" size LLM, that means it has 10 billion weights
  and takes 20GB of space at full precision.
- There are useful models at sizes from ~150M up to ~2T.

To run a model, you at least need it loaded in RAM.

Two performance constraints:

- Raw matrix multiply compute power (used to process prompt into
  embeddings tensor).
  - CPU: Cores *SSE Vector Units* Frequency
  - 256 / 16 = 16 *8 cores* 2 GHz = 256 GFLOPs
  - GPU: A fast GPU (5090) = 2 TFLOPS
- Memory bandwidth (used for token generation, as we do math on
  every weight in the network).
  - Dual channel DDR5 - Bandwidth per channel * channels
  - 1 channel of DDR5 ~ 50 GBPS * 2 = 100 GBPS
  - Fast GPU (5090) = 2 GBPS - but only 32 GB

Workstation gear:

- Pro 6000 gets you 96 GB / 4 TFLOPS

Data center gear:

- A big server might have 12-channel DDR5 per socket
  (could have 6 TB of RAM per socket)
- A big server accelerator card might have 256 GB of HBM3

## How big a model do we need

- Very small: Under 5B
- Small: Under 20B
- Medium: Under 60B
- Large: Under 200B
- Very large: Over 200B
