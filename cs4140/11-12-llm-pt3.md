# Large Language Models, pt2

## How do we actually run these models

- Find models on Hugging Face
- Run with Python's Transformers library.
  - Allows for training / fine-tuning.
- Run them with Llama.cpp
  - Inference only
  - Fast, efficient quants.
- Most other common options are built on those
  two cores.

## Today: Performance, Writing Code that uses LLMs

### How to get LLM in program

- For very small models, embed an inference engine directly
  in our app.
- Generally, want to do LLM inference through an API server.

Choice: Run our own LLM inference server or pay for API access?

Advantages to using an inference service:

- It's easier, at least to get started.
- Much cheaper to get started.
- Want to use non-open-source model.
- Reliability
- Can get access to very powerful hardware

Advantages to running your own inference server:

- Control
  - You can fix your own issues
  - You get to pick exactly which model you're running, including
    fine tunes and quantization level
- No getting throttled
- No extra bills
  - A feeling of freedom in experimenting
- Privacy / Compliance
- You know how all the pieces work and can actually figure out
  what you're doing

## Programming with LLMs

Want to write a function.

Example: get_nutrient_info("Four bags of potato chips") => {protein: "0.5g", ...
}

- Infeasible to do with simple algorithm.
- But LLM could do it.
- Problem: LLM doesn't have all nutrition data trained in.
- LLM needs that data.
- Problem: Context length isn't big enough to just put in whole food DB.
- Solution:
  - Retrieval augmented generation (RAG), where we tell the LLM about a
    data collection and how to access stuff in it.
- Problem: LLMs just do completions, or chats.
- Solution: Tool calling
  - LLMs are trained to be given a list of "tools" or functions that they
    can call, and they can call a function instead of returning a chat
    response.
