# Large Language Models, pt2

Before we talk about using LLMs, let's talk about
how to run them.

## How big?

- How many weights?
- Each weight is a floating point number, normalized to -1..1
- Standard "full precision" for weights in 2025 is 16 bit floats,
  or two bytes per weight.
- So if you see a "10B" size LLM, that means it has 10 billion weights
  and takes 20GB of space at full precision.
- There are useful models at sizes from ~150M up to ~2T.

To run a model, you at least need it loaded in RAM.

Two performance constraints:

- Raw matrix multiply compute power (used to process prompt into
  embeddings tensor).
  - CPU: Cores *SSE Vector Units* Frequency
  - 256 / 16 = 16 *8 cores* 2 GHz = 256 GFLOPs
  - GPU: A fast GPU (5090) = 2 TFLOPS
- Memory bandwidth (used for token generation, as we do math on
  every weight in the network).
  - Dual channel DDR5 - Bandwidth per channel * channels
  - 1 channel of DDR5 ~ 50 GBPS * 2 = 100 GBPS
  - Fast GPU (5090) = 2 GBPS - but only 32 GB

Workstation gear:

- Pro 6000 gets you 96 GB / 4 TFLOPS

Data center gear:

- A big server might have 12-channel DDR5 per socket
  (could have 6 TB of RAM per socket)
- A big server accelerator card might have 256 GB of HBM3

## How big a model do we need

- Very small: Under 5B
  - Useful for basic language processing, especially for simple and well
    defined tasks.
  - e.g. Convert a slightly out-of-format CSV file to JSON.
  - Will run basically anywhere: In a browser on a phone, on small
    single board computer.
  - May even accelerate with one of those NPUs hardware vendors like
    to put everywhere.
- Small: Under 20B
  - Qwen3 7B, or one of the 14B models.
  - Can work well for simple to moderate tasks.
  - Can run on a "normal" PC or on a gaming GPU.
- Medium: Up to ~70B
  - Generally useful
  - Needs "pro" hardware to run (e.g. $2k of computer)
- Large: Under 200B
  - Very useful.
  - Needs expensive "pro" hardware to run (e.g. $4k of computer)
- Very large: Over 200B
  - You'll want a decent server.
  - Proprietary "frontier" models fall in this category.

## Quantization

- Lossy compression to represent our model in less bits.
- Simple quantization is *really* lossy.
- LLM quantization is a bit more clever:
  - Block scheme
  - Block of 32 or 256 weights, each stored as a Q-bit number (less than 16).
  - At the beginning of each block, a block base, stored at full precision.
  - The actual value of a weight is f(base, specific weight)
- Common quantizations / precisions:
  - Full precision: FP16 or BF16
  - fp8, mxfp4
  - Q8_0, Q4_0, Q8_K, Q4_K, ..., Q3_, IQ_2, IQ\_1
- How low can you go and have it still work?
  - 8 bits per weight is almost as good as full 16
  - 4 is pretty good
  - less than 4 gets bad fast
- More weights or a higher quant?
  - Optimal is somewhere between Q4 and Q8, with the most
    weights you can get.

## How do we actually run these models

Today: Get them from Hugging Face

Default: The Python transformers library.
